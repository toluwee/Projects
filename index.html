<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Fine-Tuning GPT-2 Guide</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet"/>
  <link href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.css" rel="stylesheet"/>
  <style>
    body {
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 2rem;
      background-color: #f9f9f9;
      color: #222;
    }
    h1, h2 {
      color: #0f172a;
    }
    .blog-post {
      max-width: 900px;
      margin: auto;
    }
    pre {
      background: #2d2d2d;
      padding: 1em;
      border-radius: 8px;
      overflow-x: auto;
    }
    code {
      font-size: 0.95em;
    }
    ul {
      margin-left: 1.5em;
    }
  </style>
</head>
<body>
<div class="blog-post">
<h1>Fine-Tuning GPT-2 with Hugging Face Transformers: Complete Guide</h1>
<p>Learn how to adapt GPT-2 for specific tasks using custom data...</p>

<h2>Prerequisites</h2>
<pre><code class="language-bash">pip install transformers datasets torch accelerate</code></pre>
<p>Requirements: Python 3.8+, GPU with 8GB+ VRAM</p>

<h2>1. Preparing Your Dataset</h2>
<pre><code class="language-python">from datasets import load_dataset

dataset = load_dataset('text', data_files={'train': 'train.txt', 'valid': 'valid.txt'})</code></pre>

<h2>2. Tokenization</h2>
<pre><code class="language-python">from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token  # Critical for GPT-2

def tokenize_function(examples):
    return tokenizer(examples['text'], 
                   truncation=True,
                   max_length=512,
                   padding='max_length')

tokenized_data = dataset.map(tokenize_function, batched=True)</code></pre>

<h2>3. Model Initialization</h2>
<pre><code class="language-python">from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained('gpt2')</code></pre>

<h2>4. Training Setup</h2>
<pre><code class="language-python">from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,  # Reduce memory usage
    learning_rate=2e-5,
    fp16=True  # Enable mixed precision training
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data['train'],
    eval_dataset=tokenized_data['valid']
)</code></pre>

<h2>5. Start Training</h2>
<pre><code class="language-python">trainer.train()
</code></pre>

<h2>6. Evaluation & Saving</h2>
<pre><code class="language-python">results = trainer.evaluate()
print(f"Validation Loss: {results['eval_loss']}")

model.save_pretrained('fine-tuned-gpt2')
tokenizer.save_pretrained('fine-tuned-gpt2')
</code></pre>

<h2>Pro Tips</h2>
<ul>
<li>Use gradient checkpointing for large batches</li>
<li>Implement early stopping with patience=2</li>
<li>Try smaller variants like DistilGPT-2 for faster training</li>
</ul>
</div>
</body>

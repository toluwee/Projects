<!-- HTML Version -->
<div class="blog-post">
<h1>Fine-Tuning GPT-2 with Hugging Face Transformers: Complete Guide</h1>
<p>Learn how to adapt GPT-2 for specific tasks using custom data...</p>

<h2>Prerequisites</h2>
<pre><code class="language-bash">pip install transformers datasets torch accelerate
</code></pre>
<p>Requirements: Python 3.8+, GPU with 8GB+ VRAM</p>

<h2>1. Preparing Your Dataset</h2>
<p>Format your data as plain text files:</p>
<pre><code class="language-python">from datasets import load_dataset

dataset = load_dataset('text', data_files={'train': 'train.txt', 'valid': 'valid.txt'})
</code></pre>

<h2>2. Tokenization</h2>
<pre><code class="language-python">from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token  # Critical for GPT-2 [3]

def tokenize_function(examples):
    return tokenizer(examples['text'], 
                   truncation=True,
                   max_length=512,
                   padding='max_length')

tokenized_data = dataset.map(tokenize_function, batched=True)
</code></pre>

<h2>3. Model Initialization</h2>
<pre><code class="language-python">from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained('gpt2')
</code></pre>

<h2>4. Training Setup</h2>
<pre><code class="language-python">from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,  # Reduce memory usage [5]
    learning_rate=2e-5,
    fp16=True  # Enable mixed precision training
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data['train'],
    eval_dataset=tokenized_data['valid']
)
</code></pre>

<h2>5. Start Training</h2>
<pre><code class="language-python">trainer.train()
</code></pre>

<h2>6. Evaluation & Saving</h2>
<pre><code class="language-python">results = trainer.evaluate()
print(f"Validation Loss: {results['eval_loss']}")

model.save_pretrained('fine-tuned-gpt2')
tokenizer.save_pretrained('fine-tuned-gpt2')
</code></pre>

<h2>Pro Tips</h2>
<ul>
<li>Use gradient checkpointing for large batches</li>
<li>Implement early stopping with patience=2</li>
<li>Try smaller variants like DistilGPT-2 for faster training</li>
</ul>
</div>
